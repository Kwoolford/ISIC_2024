{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36ce12d3-bcbf-40c3-b9f0-ae3623af5999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before one-hot encoding, shape: (264438, 6)\n",
      "Before one-hot encoding, columns: Index(['age_approx', 'sex', 'anatom_site_general', 'clin_size_long_diam_mm',\n",
      "       'image_type', 'tbp_tile_type'],\n",
      "      dtype='object')\n",
      "After one-hot encoding, shape: (264438, 14)\n",
      "After one-hot encoding, columns: Index(['age_approx', 'clin_size_long_diam_mm', 'sex_female', 'sex_male',\n",
      "       'sex_nan', 'anatom_site_general_anterior torso',\n",
      "       'anatom_site_general_head/neck', 'anatom_site_general_lower extremity',\n",
      "       'anatom_site_general_posterior torso',\n",
      "       'anatom_site_general_upper extremity', 'anatom_site_general_nan',\n",
      "       'image_type_TBP tile: close-up', 'tbp_tile_type_3D: XP',\n",
      "       'tbp_tile_type_3D: white'],\n",
      "      dtype='object')\n",
      "Before one-hot encoding, shape: (60159, 6)\n",
      "Before one-hot encoding, columns: Index(['age_approx', 'sex', 'anatom_site_general', 'clin_size_long_diam_mm',\n",
      "       'image_type', 'tbp_tile_type'],\n",
      "      dtype='object')\n",
      "After one-hot encoding, shape: (60159, 14)\n",
      "After one-hot encoding, columns: Index(['age_approx', 'clin_size_long_diam_mm', 'sex_female', 'sex_male',\n",
      "       'sex_nan', 'anatom_site_general_anterior torso',\n",
      "       'anatom_site_general_head/neck', 'anatom_site_general_lower extremity',\n",
      "       'anatom_site_general_posterior torso',\n",
      "       'anatom_site_general_upper extremity', 'anatom_site_general_nan',\n",
      "       'image_type_TBP tile: close-up', 'tbp_tile_type_3D: XP',\n",
      "       'tbp_tile_type_3D: white'],\n",
      "      dtype='object')\n",
      "Before one-hot encoding, shape: (100265, 6)\n",
      "Before one-hot encoding, columns: Index(['age_approx', 'sex', 'anatom_site_general', 'clin_size_long_diam_mm',\n",
      "       'image_type', 'tbp_tile_type'],\n",
      "      dtype='object')\n",
      "After one-hot encoding, shape: (100265, 14)\n",
      "After one-hot encoding, columns: Index(['age_approx', 'clin_size_long_diam_mm', 'sex_female', 'sex_male',\n",
      "       'sex_nan', 'anatom_site_general_anterior torso',\n",
      "       'anatom_site_general_head/neck', 'anatom_site_general_lower extremity',\n",
      "       'anatom_site_general_posterior torso',\n",
      "       'anatom_site_general_upper extremity', 'anatom_site_general_nan',\n",
      "       'image_type_TBP tile: close-up', 'tbp_tile_type_3D: XP',\n",
      "       'tbp_tile_type_3D: white'],\n",
      "      dtype='object')\n",
      "Data loaders and datasets created successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import hashlib\n",
    "import pickle\n",
    "import multiprocessing\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                    format='(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "                    handlers=[logging.StreamHandler()])\n",
    "\n",
    "# Data paths and parameters\n",
    "METADATA_PATH = '..\\\\Data\\\\train-metadata.csv'\n",
    "IMAGE_DIR = '..\\\\Data\\\\lanczos_train_image\\\\image'\n",
    "CACHE_DIR ='../dataset_cache'\n",
    "BATCH_SIZE = 32\n",
    "OVERSAMPLING_RATIO = 0.1 # .1 = 1:10, .2 = 1:5\n",
    "    \n",
    "# Due to different datatype, need to separate features for processing\n",
    "FEATURES = ['age_approx', 'sex', 'anatom_site_general', 'clin_size_long_diam_mm', 'image_type', 'tbp_tile_type']\n",
    "NUMERIC_FEATURES = ['age_approx', 'clin_size_long_diam_mm']\n",
    "CATEGORICAL_FEATURES = ['sex', 'anatom_site_general', 'image_type', 'tbp_tile_type']\n",
    "\n",
    "def load_and_preprocess_metadata(metadata_path):\n",
    "    \"\"\"Loads and preprocesses the metadata.\"\"\"\n",
    "    df = pd.read_csv(metadata_path, low_memory=False)\n",
    "\n",
    "    if 'isic_id' not in df.columns:\n",
    "        raise ValueError(\"CSV file does not contain 'isic_id' column\")\n",
    "\n",
    "    df['image_filename'] = df['isic_id'] + '.jpg'\n",
    "\n",
    "    # Fill missing values\n",
    "    df['age_approx'].fillna(df['age_approx'].mean())\n",
    "    df['sex'].fillna('unknown')\n",
    "    df['anatom_site_general'].fillna('unknown')\n",
    "\n",
    "    return df\n",
    "\n",
    "def prepare_features(df):\n",
    "    \"\"\"Prepares features for the model.\"\"\"\n",
    "\n",
    "    print(\"Before one-hot encoding, shape:\", df.shape)\n",
    "    print(\"Before one-hot encoding, columns:\", df.columns)\n",
    "    \n",
    "    df[NUMERIC_FEATURES] = df[NUMERIC_FEATURES].fillna(df[NUMERIC_FEATURES].mean())\n",
    "\n",
    "    # One-hot encode categorical features\n",
    "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    encoded_features = encoder.fit_transform(df[CATEGORICAL_FEATURES])\n",
    "    encoded_feature_names = encoder.get_feature_names_out(CATEGORICAL_FEATURES)\n",
    "\n",
    "    # Combine numeric and encoded features\n",
    "    all_feature_names = NUMERIC_FEATURES + list(encoded_feature_names)\n",
    "    feature_array = np.hstack((df[NUMERIC_FEATURES].values, encoded_features))\n",
    "    features_df = pd.DataFrame(feature_array, columns=all_feature_names)\n",
    "    print(\"After one-hot encoding, shape:\", features_df.shape)\n",
    "    print(\"After one-hot encoding, columns:\", features_df.columns)\n",
    "    return features_df, all_feature_names\n",
    "\n",
    "# Define the Dataset class\n",
    "class SkinLesionDataset(Dataset):\n",
    "    def __init__(self, image_dir, dataframe, features_df, features, transform=None, use_cache=True, cache_dir='./dataset_cache'):\n",
    "        self.image_dir = image_dir\n",
    "        self.dataframe = dataframe\n",
    "        self.features_df = features_df\n",
    "        self.features = features\n",
    "        self.transform = transform\n",
    "        self.use_cache = use_cache\n",
    "        self.cache_dir = cache_dir\n",
    "\n",
    "        if self.use_cache:\n",
    "            os.makedirs(self.cache_dir, exist_ok =True)\n",
    "\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # print(f\"DataFrame length: {len(self.dataframe)}, Accessing index: {idx}\")  # Debugging\n",
    "\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            if self.use_cache:\n",
    "                cache_path = self._get_cache_path(idx)\n",
    "                if os.path.exists(cache_path):\n",
    "                    # with multiprocessing.Pool() as pool:\n",
    "                    #     item = pool.apply(self._load_from_cache, args=(cache_path,)) # load in parallel\n",
    "                    item = self._load_from_cache(cache_path)\n",
    "                    # self.logger.debug(f\"Cache Load Time: {time.time() - start_time:.4f} seconds\")\n",
    "                    # print(f\"Cache Load Time: {time.time() - start_time:.4f} seconds\")\n",
    "                    return item\n",
    "                else: # Does not exist, load and save\n",
    "                    load_start_time = time.time()\n",
    "                    image, features, target = self._load_item(idx)\n",
    "                    # print(f\"Item Load Time: {time.time() - load_start_time:.4f} seconds\")\n",
    "                    # self.logger.debug(f\"Item Load Time: {time.time() - load_start_time:.4f}: seconds\")\n",
    "                    \n",
    "                    transform_start_time = time.time()\n",
    "                    if target == 1:\n",
    "                        image = self.positive_transforms(image)\n",
    "                    else:\n",
    "                        image = self.negative_transforms(image)\n",
    "                    # print(f\"Transform Time: {time.time - transform_start_time:.4f} seconds\")\n",
    "                    # self.logger.debug(f\"Transform Time: {time.time() - start_time:.4f} seconds\")\n",
    "        \n",
    "                    cache_start_time = time.time()\n",
    "                    item = (image, features, target)\n",
    "                    # with multiprocessing.Pool as pool:\n",
    "                    #     pool.apply_async(self._save_to_cache, args=(cache_path, item))\n",
    "                    self._save_to_cache(cache_path, item)\n",
    "                    # print(f\"Cache Save Time: {time.time() - cache_start_time:.4f} seconds\")\n",
    "                    # self.logger.debug(f\"Cache Save Time: {time.time() - cache_start_time:4.f} seconds\")\n",
    "        \n",
    "            else: # Caching disabled, just load and transform\n",
    "                load_start_time = time.time()\n",
    "                image, features, target = self._load_item(idx)\n",
    "                # print(f\"Item Load Time: {time.time() - load_start_time:.4f} seconds\")\n",
    "                # self.logger.debug(f\"Item Load Time: {time.time() - load_start_time:.4f} seconds\")\n",
    "        \n",
    "                transform_start_time = time.time()\n",
    "                if target == 1:\n",
    "                    image = self.positive_transforms(image)\n",
    "                else:\n",
    "                    image = self.negative_transforms(image)\n",
    "                # print(f\"Transform Time: {time.time() - transform_start_time:.4f} seconds\")\n",
    "                # self.logger.debug(f\"Transform Time: {time.time() - transform_start_time} seconds\")\n",
    "        \n",
    "            # print(f\"Total GetItem Time: {time.time() - start_time:.4f} seconds\")\n",
    "            # self.logger.debug(f\"Total GetItem Time: {time.time() - start_time:.4f} seconds\")\n",
    "            return image, features, target\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting item at index {idx}: {e}\")\n",
    "            set.logger.error(f\"Error getting item at index {idx}: {e}\")\n",
    "            raise\n",
    "\n",
    "    @staticmethod\n",
    "    def positive_transforms(image):\n",
    "        positive_transforms = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            transforms.RandomRotation(30),\n",
    "            transforms.ColorJitter(brightness=0.2,\n",
    "                                   contrast=0.2, saturation=0.2, hue=0.1)\n",
    "        ])\n",
    "        return positive_transforms(image)\n",
    "    @staticmethod\n",
    "    def negative_transforms(image):\n",
    "        negative_transforms = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "        ])\n",
    "        return negative_transforms(image)\n",
    "        \n",
    "    def _load_item(self, idx):\n",
    "        try:\n",
    "            load_start = time.time()\n",
    "            img_name = os.path.join(self.image_dir, self.dataframe.iloc[idx]['image_filename'])\n",
    "            image = Image.open(img_name)\n",
    "            # print(f\"Image Load Time: {time.time() - load_start}\")\n",
    "            if self.transform:\n",
    "                transform_start = time.time()\n",
    "                image = self.transform(image)\n",
    "                # print(f\"Transform Time: {time.time() - transform_start}\")\n",
    "    \n",
    "            feature_time = time.time()\n",
    "            features = torch.tensor(self.features_df.iloc[idx].values, dtype=torch.float)\n",
    "            # print(f\"Feature Time: {time.time() - feature_time}\")\n",
    "            target_time = time.time()\n",
    "            target = torch.tensor(self.dataframe.iloc[idx]['target'], dtype=torch.long)\n",
    "            # print(f\"Target Time: {time.time() - target_time}\")\n",
    "            return image, features, target\n",
    "            # self.logger.debug(\"Image Load Time: {time.time() - load_start}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading item at index {idx}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _get_cache_path(self, idx):\n",
    "        filename = f\"{idx}_{self.dataframe.iloc[idx]['image_filename']}\"\n",
    "        hashed_filename = hashlib.md5(filename.encode()).hexdigest()\n",
    "        return os.path.join(self.cache_dir, f\"{hashed_filename}.pkl\")\n",
    "\n",
    "    def _save_to_cache(self, cache_path, item):\n",
    "        try:\n",
    "            with open(cache_path, 'wb') as f:\n",
    "                pickle.dump(item, f)\n",
    "        except Exception as e:\n",
    "            # self.logger.error(f\"Error Saving item to disk at index {idx}: {e}\")\n",
    "            raise\n",
    "            \n",
    "    def _load_from_cache(self, cache_path):\n",
    "        try:\n",
    "            with open(cache_path, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "        except Exception as e:\n",
    "            # self.logger.error(f\"Error loading item from cache at index {idx}: {e}\")\n",
    "            raise\n",
    "    def preload(self):\n",
    "        for i in tqdm(range(len(self)), desc='Preloading Data'):\n",
    "            _ = self[i]\n",
    "        # print(f'Preloaded {len(self)} items into cache.')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess metadata\n",
    "    df = load_and_preprocess_metadata(METADATA_PATH)\n",
    "\n",
    "    # Split data into features and target\n",
    "    features_data = df[FEATURES]\n",
    "    target_labels = df['target']\n",
    "\n",
    "    # Initial train-test split (before oversampling)\n",
    "    # Create a test set that represents the true distribution\n",
    "    features_train_val, features_test, target_train_val, target_test = train_test_split(\n",
    "        features_data, target_labels, test_size=0.25, random_state=42, stratify=target_labels\n",
    "    )\n",
    "\n",
    "    # Split the remaining data into training and validation sets\n",
    "    features_train, features_val, target_train, target_val = train_test_split(\n",
    "        features_train_val, target_train_val, test_size=0.2, random_state=42, stratify=target_train_val\n",
    "    )\n",
    "\n",
    "    # Oversample the minority class in the training set only\n",
    "    oversampler = RandomOverSampler(sampling_strategy=OVERSAMPLING_RATIO, random_state=42)\n",
    "    features_train_resampled, target_train_resampled = oversampler.fit_resample(features_train, target_train)\n",
    "\n",
    "    # Prepare features for each split\n",
    "    train_features, all_features = prepare_features(features_train_resampled)\n",
    "    val_features, _ = prepare_features(features_val)\n",
    "    test_features, _ = prepare_features(features_test)\n",
    "\n",
    "    # Image transformation\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((160, 160)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Reconstruct DataFrames for each set\n",
    "    train_df = pd.concat([features_train_resampled, target_train_resampled, df['image_filename']], axis=1)\n",
    "    val_df = pd.concat([features_val, target_val, df['image_filename']], axis=1)\n",
    "    test_df = pd.concat([features_test, target_test, df['image_filename']], axis=1)\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = SkinLesionDataset(IMAGE_DIR, train_df[:len(features_train_resampled)], train_features, all_features, transform=transform, use_cache=True, cache_dir='./train_cache')\n",
    "    val_dataset = SkinLesionDataset(IMAGE_DIR, val_df[:len(features_val)], val_features, all_features, transform=transform, use_cache=True, cache_dir='/val_cache')\n",
    "    test_dataset = SkinLesionDataset(IMAGE_DIR, test_df[:len(features_test)], test_features, all_features, transform=transform, use_cache=True, cache_dir='./test_cache')\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "    print('Data loaders and datasets created successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29003b9f-270e-4d2c-9f96-d5d54887af95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0019b24-9233-428f-9106-47c21d09af30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preload data\n",
    "# print(\"Preloading training data...\")\n",
    "# train_dataset.preload()\n",
    "# print(\"Preloading validation data...\")\n",
    "# val_dataset.preload()\n",
    "# print(\"Preloading test data...\")\n",
    "# test_dataset.preload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b143ae4-8533-4b9f-ad33-d42c6ffcb81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "class SkinLesionModel(nn.Module):\n",
    "    def __init__(self, num_classes, num_features):\n",
    "        super(SkinLesionModel, self).__init__()\n",
    "\n",
    "        # Image feature extractor (pretrained resnet model)\n",
    "        self.resnet = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        self.resnet.fc = nn.Identity() # Remove the final fully connected layer\n",
    "    \n",
    "        # Freeze the parameters of the ResNet\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "        # Additional features processing\n",
    "        self.feature_fc = nn.Sequential(\n",
    "            nn.Linear(num_features, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "    \n",
    "        # Combine image features and additional features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2048 + 64, 512), # 2048 from ResNet50, 64 from additional features\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    def forward(self, image, features):\n",
    "        # Process image through ResNet\n",
    "        # Input shape: [batch_size, 3, 160, 160]\n",
    "        # Output shape: [batch_size, 2048]\n",
    "        image_features = self.resnet(image)\n",
    "\n",
    "        # Process additional features\n",
    "        # Input shape: [batch_size, num_features]\n",
    "        # Output shape: [batch_size, 64]\n",
    "        processed_features = self.feature_fc(features)\n",
    "\n",
    "        # Combine features\n",
    "        # Output shape: [batch_size, 2048 + 64]\n",
    "        combined_features = torch.cat((image_features, processed_features), dim=1)\n",
    "\n",
    "        # Final classification\n",
    "        # Output shape: [batch_size, num_classes]\n",
    "        output = self.classifier(combined_features)\n",
    "\n",
    "        return output\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, device='cuda'):\n",
    "    model.to(device)\n",
    "    print(f'Sending model to device: {device}')\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 60)\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        train_positives = 0\n",
    "        train_true_positives = 0\n",
    "        train_false_positives = 0\n",
    "        train_predictions = []\n",
    "        train_targets = []\n",
    "\n",
    "        # print(\"Checking Data Loader...\")\n",
    "        # for i, (images, features, targets) in enumerate(train_loader):\n",
    "        #     if i == 0:\n",
    "        #         print(f'First batch loaded. Shapes: images: {images.shape}, features {features.shape}, targets {targets.shape}')\n",
    "        #         break\n",
    "        # print(\"Data Loader Check Complete\")\n",
    "        \n",
    "        print('Training:')\n",
    "        progress_bar = tqdm(train_loader, desc='Training')\n",
    "        for batch_idx, (images, features, targets) in enumerate(progress_bar):\n",
    "            # print(f'Processing batch {batch_idx+1}/{len(train_loader)}')\n",
    "            # print(f'Images shape: {images.shape}, Features shape: {features.shape}, Targets shape: {targets.shape}')\n",
    "            images = images.to(device)\n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, features)\n",
    "            loss = criterion(outputs, targets.float().unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            predictions = (torch.sigmoid(outputs) > 0.5).squeeze()\n",
    "            train_correct += (predictions == targets).sum().item()\n",
    "            train_total += targets.size(0)\n",
    "\n",
    "            batch_positives = predictions.sum().item()\n",
    "            train_positives += batch_positives\n",
    "            train_true_positives += ((predictions == 1) & (targets == 1)).sum().item()\n",
    "            train_false_positives += ((predictions == 1) & (targets == 0)).sum().item()\n",
    "\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{train_correct/train_total:.4f}',\n",
    "                'pos_pred': f'{batch_positives}/{targets.size(0)}'\n",
    "            })\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_accuracy = train_correct / train_total\n",
    "        train_precision = train_true_positives / (train_positives + 1e-8)\n",
    "        train_recall = train_true_positives / (targets.sum().item() + 1e-8)\n",
    "        train_f1 = 2 * (train_precision * train_recall) / (train_precision + train_recall + 1e-8)\n",
    "            \n",
    "        print('\\nTraining Results:')\n",
    "        print(f'Loss: {train_loss:.4f}, Accuracy: {train_accuracy}')\n",
    "        print(f'Total Predictions: {train_total}, Positive Predictions: {train_positives}')\n",
    "        print(f'True Positives: {train_true_positives}, False Positives: {train_false_positives}')\n",
    "        print(f'Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}')\n",
    "        \n",
    "\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_predictions = []\n",
    "        val_targets = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            progress_bar = tqdm(val_loader, desc='Validation')\n",
    "            for batch_idx, (images, features, targets) in enumerate(progress_bar):\n",
    "                images = images.to(device)\n",
    "                features = features.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                outputs = model(images, features)\n",
    "                loss = criterion(outputs, targets.float().unsqueeze(1))\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                predictions = (torch.sigmoid(outputs) > 0.5).squeeze()\n",
    "                val_correct += (predictions == targets).sum().item()\n",
    "                val_total += targets.size(0)\n",
    "\n",
    "                val_predictions.extend(predictions.cpu().numpy())\n",
    "                val_targets.extend(targets.cpu().numpy())\n",
    "                \n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': f'{loss.item():.4f}',\n",
    "                    'acc': f'{val_correct/val_total:.4f}'\n",
    "                })\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = val_correct / val_total\n",
    "        val_f1 = f1_score(val_targets, val_predictions)\n",
    "        val_confusion_matrix = confusion_matrix(val_targets, val_predictions)\n",
    "\n",
    "        print(f'\\nValidation Results:')\n",
    "        print(f'Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}, F1: {val_f1:.4f}')\n",
    "        print('Confusion Matrix:')\n",
    "        print(val_confusion_matrix)\n",
    "        print('-' * 60)\n",
    "\n",
    "    print(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6df8d5f-79b2-4baf-bf11-9e9ea80551b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Hyperparameters\n",
      "Device = cuda\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "# Hyperparameters and setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Setting up Hyperparameters\\nDevice = {device}\")\n",
    "num_classes = 1\n",
    "num_features = len(all_features)\n",
    "batch_size = 32\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Create model, loss function, and optimizer\n",
    "model = SkinLesionModel(num_classes, num_features)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f30c20ea-cd01-49b3-b028-72d559eece50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending model to device: cuda\n",
      "\n",
      "Epoch 1/5\n",
      "------------------------------------------------------------\n",
      "Training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 7748/7748 [32:06<00:00,  4.02it/s, loss=0.0002, acc=0.9932, pos_pred=2/7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Results:\n",
      "Loss: 0.0295, Accuracy: 0.9931991722836018\n",
      "Total Predictions: 247911, Positive Predictions: 21675\n",
      "True Positives: 21263, False Positives: 412\n",
      "Precision: 0.9810, Recall: 10631.4999, F1: 1.9618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████████████| 1292/1292 [05:11<00:00,  4.14it/s, loss=0.0001, acc=0.9968]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Results:\n",
      "Loss: 0.0147, Accuracy: 0.9968, F1: 0.9823\n",
      "Confusion Matrix:\n",
      "[[37557     5]\n",
      " [  126  3630]]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 2/5\n",
      "------------------------------------------------------------\n",
      "Training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 7748/7748 [32:20<00:00,  3.99it/s, loss=0.0006, acc=0.9958, pos_pred=1/7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Results:\n",
      "Loss: 0.0197, Accuracy: 0.9957847776016393\n",
      "Total Predictions: 247911, Positive Predictions: 21954\n",
      "True Positives: 21723, False Positives: 231\n",
      "Precision: 0.9895, Recall: 21722.9998, F1: 1.9789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████████████| 1292/1292 [05:09<00:00,  4.17it/s, loss=0.0007, acc=0.9969]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Results:\n",
      "Loss: 0.0120, Accuracy: 0.9969, F1: 0.9827\n",
      "Confusion Matrix:\n",
      "[[37499    63]\n",
      " [   67  3689]]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 3/5\n",
      "------------------------------------------------------------\n",
      "Training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 7748/7748 [32:17<00:00,  4.00it/s, loss=0.0001, acc=0.9964, pos_pred=1/7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Results:\n",
      "Loss: 0.0179, Accuracy: 0.9963777323313608\n",
      "Total Predictions: 247911, Positive Predictions: 21997\n",
      "True Positives: 21818, False Positives: 179\n",
      "Precision: 0.9919, Recall: 21817.9998, F1: 1.9836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████████████| 1292/1292 [05:12<00:00,  4.14it/s, loss=0.0001, acc=0.9973]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Results:\n",
      "Loss: 0.0171, Accuracy: 0.9973, F1: 0.9848\n",
      "Confusion Matrix:\n",
      "[[37550    12]\n",
      " [  101  3655]]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 4/5\n",
      "------------------------------------------------------------\n",
      "Training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 7748/7748 [32:19<00:00,  3.99it/s, loss=0.0000, acc=0.9969, pos_pred=1/7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Results:\n",
      "Loss: 0.0153, Accuracy: 0.9969464848272163\n",
      "Total Predictions: 247911, Positive Predictions: 22064\n",
      "True Positives: 21922, False Positives: 142\n",
      "Precision: 0.9936, Recall: 21921.9998, F1: 1.9870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████████████| 1292/1292 [05:15<00:00,  4.10it/s, loss=0.0001, acc=0.9963]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Results:\n",
      "Loss: 0.0152, Accuracy: 0.9963, F1: 0.9798\n",
      "Confusion Matrix:\n",
      "[[37474    88]\n",
      " [   64  3692]]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 5/5\n",
      "------------------------------------------------------------\n",
      "Training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 7748/7748 [32:50<00:00,  3.93it/s, loss=0.0012, acc=0.9964, pos_pred=0/7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Results:\n",
      "Loss: 0.0186, Accuracy: 0.9964019345652271\n",
      "Total Predictions: 247911, Positive Predictions: 22015\n",
      "True Positives: 21830, False Positives: 185\n",
      "Precision: 0.9916, Recall: 2183000000000.0000, F1: 1.9832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████████████| 1292/1292 [05:16<00:00,  4.09it/s, loss=0.0003, acc=0.9978]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Results:\n",
      "Loss: 0.0093, Accuracy: 0.9978, F1: 0.9879\n",
      "Confusion Matrix:\n",
      "[[37554     8]\n",
      " [   82  3674]]\n",
      "------------------------------------------------------------\n",
      "Training Complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5f44463-5793-4f98-ac64-04993cd286fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: ..\\Model_Weights\\Attempt_1\\evaluation.pth\n"
     ]
    }
   ],
   "source": [
    "# Saving the Weights (Only run if training was run)\n",
    "MODEL_SAVE_PATH = '..\\\\Model_Weights\\\\Attempt_1\\\\evaluation.pth'\n",
    "\n",
    "torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "\n",
    "print(f'Model saved to: {MODEL_SAVE_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bae7b17-706c-40a7-9984-7f1ec4e137da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Model loaded from ..\\Model_Weights\\Attempt_1\\evaluation.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "MODEL_SAVE_PATH = '..\\\\Model_Weights\\\\Attempt_1\\\\evaluation.pth'\n",
    "\n",
    "# Hyperparameters and setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "\n",
    "model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "model.to(device)\n",
    "print(f'Model loaded from {MODEL_SAVE_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dfd285b-2697-4131-ac52-f526b2152e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████| 3134/3134 [13:33<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9995\n",
      "Test Precision: 0.6667\n",
      "Test Recall: 0.9592\n",
      "Test F1 Score: 0.7866\n",
      "Confusion Matrix:\n",
      "[[100120     47]\n",
      " [     4     94]]\n"
     ]
    }
   ],
   "source": [
    "# Testing the model on the test dataset\n",
    "model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, features, targets in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        images = images.to(device)\n",
    "        features = features.to(device)\n",
    "        outputs = model(images, features)\n",
    "        probabilities = torch.sigmoid(outputs)\n",
    "        predictions = (probabilities > 0.5).int()\n",
    "\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "accuracy = accuracy_score(all_targets, all_predictions)\n",
    "precision = precision_score(all_targets, all_predictions)\n",
    "recall = recall_score(all_targets, all_predictions)\n",
    "f1 = f1_score(all_targets, all_predictions)\n",
    "conf_matrix = confusion_matrix(all_targets, all_predictions)\n",
    "\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "print(f'Test Precision: {precision:.4f}')\n",
    "print(f'Test Recall: {recall:.4f}')\n",
    "print(f'Test F1 Score: {f1:.4f}')\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7fe329-a442-4e11-8355-14b76ca01996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle Save\n",
    "# Saving the Encoder for later use\n",
    "import pandas as pd\n",
    "\n",
    "CATEGORICAL_FEATURES = ['sex', 'anatom_site_general', 'image_type', 'tbp_tile_type']\n",
    "TRAIN_METADATA_PATH = 'train-metadata.csv'\n",
    "\n",
    "train_df_categorical = pd.read_csv(TRAIN_METADATA_PATH, usecols=CATEGORICAL_FEATURES)\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "encoder.fit(train_df_categorical)\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(encoder, f)\n",
    "\n",
    "print('pickle dumped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3c8ed45-ef8f-4ed1-845e-c69f5c8bde71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total positive samples in the original dataset:\n",
      "target\n",
      "0    400666\n",
      "1       393\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Positive samples in the initial training set (before oversampling):\n",
      "target\n",
      "0    240399\n",
      "1       236\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Positive samples in the initial test set:\n",
      "target\n",
      "0    75125\n",
      "1     7512\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Positive samples in the training set after oversampling:\n",
      "target\n",
      "0    240399\n",
      "1     24039\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Positive samples in the final training set:\n",
      "target\n",
      "0    240399\n",
      "1       236\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Positive samples in the final validation set:\n",
      "target\n",
      "0    60100\n",
      "1       59\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Positive samples in the final test set:\n",
      "target\n",
      "0    100167\n",
      "1        98\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print total positive samples in the original dataset\n",
    "print(\"Total positive samples in the original dataset:\")\n",
    "print(target_labels.value_counts())\n",
    "\n",
    "# Print positive samples in the initial train and test splits\n",
    "print(\"\\nPositive samples in the initial training set (before oversampling):\")\n",
    "print(target_train.value_counts())\n",
    "print(\"\\nPositive samples in the initial test set:\")\n",
    "print(target_temp.value_counts())\n",
    "\n",
    "# Print positive samples after oversampling the training set\n",
    "print(\"\\nPositive samples in the training set after oversampling:\")\n",
    "print(target_train_resampled.value_counts())\n",
    "\n",
    "\n",
    "# Print positive samples in each split after the final stratified split\n",
    "print(\"\\nPositive samples in the final training set:\")\n",
    "print(target_train.value_counts())\n",
    "print(\"\\nPositive samples in the final validation set:\")\n",
    "print(target_val.value_counts())\n",
    "print(\"\\nPositive samples in the final test set:\")\n",
    "print(target_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6415bd44-e05c-42e4-b007-3c1550db81cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai_env)",
   "language": "python",
   "name": "ai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
